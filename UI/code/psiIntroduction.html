<style>
img {
    display: block;
    margin: 0 auto;
}
</style>
<div id="welcome">
<p>
The following demonstrates a tool that helps data depositors release privacy preserving statistics describing a dataset by distributing a privacy budget across different possible statistical calculations. This tool is called the <i>budgeter</i>.  Users select which statistics they would like to calculate and are given estimates of how accurately each statistic can be computed. They can also redistribute their privacy budget according to which statistics they think are most valuable in their dataset. 
</p>
<p>
Below is found a brief introduction to differential privacy, the mathematical framework that provides the privacy guarantees, and an overview of all the features and requirements of this tool.  You can bring this explanatory page back at any point by clicking any of the information buttons, marked <span class="glyphicon glyphicon-question-sign" style="color:#A3CEDC;font-size: 12px;"></span>.
</p>
<br>
<br>
</div>

<div id="intro">
<h4>Introduction to differential privacy</h4>
<p>
Differential privacy is a rigorous mathematical framework for making statistical information about private datasets available. This is done in such a way that guarantees that information about specific individuals in the dataset does not leak out.
</p>
<p>
In the simplest setting, consider an algorithm that analyzes a dataset and computes statistics about it (such as the data's mean, variance, median, mode, etc.). Such an algorithm is said to be differentially private if by looking at the output, one cannot tell whether or not any individual's data was included in the original dataset. In other words, the guarantee of a differentially private algorithm is that its behavior hardly changes when a single individual joins or leaves the dataset -- anything the algorithm might output on a database containing some individual's information is almost as likely to have come from a database without that individual's information. Most notably, this guarantee holds for any individual and anyÂ dataset. Therefore, regardless of how eccentric any single individual's details are, and regardless of the details of anyone else in the database, the guarantee of differential privacy still holds. 
</p>
<p>
The definition of differential privacy emerged from a long line of work applying algorithmic ideas to the study of privacy (Dinur and Nissim `03; Dwork and Nissim `04; Blum, Dwork, McSherry, and Nissim `05), culminating with work of Dwork, McSherry, Nissim, and Smith `06.
</p>
</div>

<div id="data">
<h4>Data requirements</h4>
<ul>
<li>Your dataset must be tabular with rows corresponding to individuals and columns corresponding to variables (attributes). </li>
<li>There should not be any missing values (empty cells in the table). </li>
<li>It is assumed that the total number of observations (rows) in the dataset is public information and can be released.  This value is not treated in a privacy preserving manner.</li>
</ul>
</div>

<div id="parameters">
<h4>Privacy Loss Parameters</h4>
<p>
The level of privacy protection in differential privacy is governed by two parameters: epsilon (&epsilon;) and delta (&delta;). The smaller these numbers, the more privacy is guaranteed. However, if these numbers are too small, the released statistics will become inaccurate. 
</p>
<p>
Typical epsilon values are in the regime of .01-1. We recommend that your epsilon does not exceed 1. This table gives some intuition about the effects of different choices of epsilon (top row).</p>

<img src="../images/DPInterpretationofEpsilonTable.png" align="middle">
<p>
Imagine an adversary has prior belief about the probability that you have some trait. These are listed in the vertical column on the left. If your data were in a dataset, the table shows the new belief that that adversary could infer from looking at differentially private statistics about the dataset with differing values of epsilon. For example, if an adversary had a prior belief of 10% that you have a particular trait, then outputs from a differentially private algorithm with epsilon set to .2 could update his belief to at most 11.95%. 
</p>

<p>
In the worst case, delta is the probability that all information about the dataset is leaked. For this reason, delta is typically set to a very small number, such as the probability that somebody can break an encryption scheme. We recommend a delta of one in a million or smaller. 
</p>
<p>
For a given level of privacy protection, there are limits on how much statistical information can be released and how accurately. It is important to make judicious choices about how to use the limited budget that you have. The budgeter tool assists you in making those choices. 
</p>

 Below is Harvard's list of varying levels of sensitivity for datasets and reasonable privacy loss parameters for each level. The recommendations below are just a guideline. See <a href="http://files.vpr.harvard.edu/files/vpr-documents/files/data_classification_table_abridged_7.23.13_0.pdf">Harvard's secure data classifications</a> for more information.
   <p>
	<ol>
		<li>Public information: It is not necessary to use differential privacy for public information.</li>
		<li>Information the disclosure of which would not cause material harm, but which the University has chosen to keep confidential: (&epsilon;=1, &delta;=10<sup>-5</sup>)</li>
		<li>Information that could cause risk of material harm to individuals or the University if disclosed: (&epsilon;=.25, &delta;=10<sup>-6</sup>)</li>
		<li>Information that would likely cause serious harm to individuals or the University if disclosed: (&epsilon;=.05, &delta;=10<sup>-7</sup>)</li>
		<li>Information that would cause severe harm to individuals or the University if disclosed: It is not recommended that the PSI tool be used with such severely sensitive data.</li>
	</ol> 
   </p>
   <p>
	Set the privacy loss parameters for your dataset below:
   </p>
</div>   

<div id="functioning">
<h4>Functioning Privacy Loss Parameters</h4>
<p>
When using some of the features of this tool (Secrecy of the Sample and Reserved Budget), the global privacy guarantee (&epsilon;, &delta;) might differ from the budget that is available to spend in the current session. For example Secrecy of the Sample boosts accuracy by effectively letting you spend more than you have while actually maintaining the same privacy guarantee. Similarly, reserving budget for future users doesn't change the global privacy settings on the dataset but limits the amount that the data depositor can spend so that there is enough leftover for future users. In both cases, we represent the amount of budget that can be spent in the current session as "functioning epsilon" and "functioning delta". When relevant, these will appear next to the global epsilon and delta in the top right of the interface. The functioning privacy loss parameters may be larger than the true ones (if secrecy of the sample is being used) or smaller (if a lot of the budget is reserved for future users). 
</p>
</div>

<div id="secofsamp">
<h4>Secrecy of the Sample</h4>
<p>
If the data is a random and secret sample from a larger population of known size, then the accuracy of the released statistics can be boosted without changing the privacy guarantee.   Here, secret means that the choice of the people in the sample will not be revealed.
</p>
<p>
This boost requires an estimate of the size of the larger population. It is important to be conservative in your estimate. In other words, it is okay to underestimate but could violate privacy if you overestimate.
</p>
</div>


<div id="statistics">
<h4>Available Privacy Preserving Statistics</h4>
<p>
When using the tool you will select statistics to be released one by one. Begin by selecting the variable on which to compute a new statistic in the left panel. This will spawn a box in the center panel for that variable. Fill in the variable type (e.g. boolean, numerical, categorical) and then select the statistic from the options we currently support:
</p>
<p>
<b>Mean:</b> Computes the average value of the variable.
</p>
<p>
<b>Histogram:</b> Computes a bar graph of the different values represented in the variable. This is meant to be used with categorical variables, where values are labels and not to be interpreted as quantities. For example, a religion variable will have a discrete set of values it can take on that are not numerical. 
</p>
<p>
<b>Quantile:</b> This computes the full cumulative distribution function, or CDF, of the variable, which can then be used to calculate any desired quantile (e.g. the median or percentiles). 
</p>
</div>

<div id="metadata">
<h4>Entering Metadata</h4>
<p>
The privacy preserving algorithms to generate these statistics each require certain auxiliary information about the variables, which we refer to as <i>metadata</i>.  Different statistics require different metadata.  The tool will only ask for the metadata needed to complete the set of statistics presently chosen by the user.  Here are the meanings of these metadata values:
</p>
<p>
<b>Upper Bound:</b> The largest value that this variable can take on. Any observations in the raw data beyond this value will be replaced with this bound (sometimes this is referred to as <i>top-coding</i> or <i>censoring</i>).  For example, if the upper bound on age is entered as 100, and a person in the data had a recorded age of 105, this observation would be overridden to the value of 100 for the calculation of any statistics.
</p>
<p>
<b>Lower Bound:</b> The smallest value that this variable can take on. Any observations in the raw data below this value will be replaced with this bound.
</p>
<p>
<b>Granularity:</b> The minimum positive distance between two different records. For example, if income is reported to the nearest $100, then the granularity of the variable is 100. If income is reported exactly to the cent, then granularity is .01. 
</p>
<p>
<b>Number of Bins:</b> The number of different categories present in the data. For example, yes or no questions have 2 bins. 
</p>
<p>
<b>Bin names:</b> Comma separated list of all possible categories the variable can take on. For example: red, white, blue. You can also use the shorthand 2:5 to represent 2, 3, 4, 5 or C:G to represent C, D, E, F, G for example. 
</p>
<p>
Remember that it is important to fill in the metadata <i>as though you have never seen the data</i>. For example, if there is an age variable in the dataset, a lower bound of 0 and an upper bound of 100 are reasonable guesses for the range of ages in any dataset.  If on the other hand, it is public information that the dataset is a survey of US voters, then putting an age lower bound of 18 would be appropriate. Do not look at the dataset to find the oldest or youngest person and record his or her age in the metadata. If this task feels too unnatural, we recommend asking a research assistant to fill in the metadata. This should be somebody who has never seen the raw data but is familiar with how it was collected and the names of the variables. 
</p>
</div>

<div id="accuracy">
<h4>Accuracy and Alpha</h4>
<p>
Every statistic that is added has an associated accuracy value. This is a theoretical bound on the worst-case error for the current parameters set for the statistic. There is a alpha (&alpha;) parameter that represents the probability that the error in the final computed statistics exceeds the error bound listed in the table. Alpha is set at a default of .05 and can be changed. The interpretation of these accuracies differs across statistics. If "accuracy" is the number reported in the accuracy column of the table and n is the number of people in the dataset, then the accuracies should be interpreted in the following ways for each statistic: 
</p>
<p>
<b>Mean:</b>   With probability 1-&alpha;,   true mean - accuracy*n &le; output mean  &le; true mean + accuracy*n
</p>
<p>
<b>Quantiles:</b> With probability 1-&alpha; for every <i>t</i>, the algorithm's output count of the number of data points less than <i>t</i> satisfies the following:    true count - accuracy*n &le; output count &le; true count + accuracy*n
</p>
<p>
<b>Histogram:</b>  With probability 1-&alpha; for each bar in the outputted bar graph,  true count-accuracy*n &le; output count on that bar &le; true count + accuracy*n
</p>
<p>
At any time you may request that certain statistics be made more or less accurate by editing the accuracy column. This will result in a reapportioning of  the global epsilon across all of the statistics. There are limits to how accurate the statistics can be. This tool uses an optimal composition theorem for differential privacy to ensure that the statistics are as accurate as possible while maintaining the global privacy guarantee. 
</p>
</div>

<div id="reserve">
<h4>Reserving Privacy Budget</h4>
<p>
PSI gives data depositors the option to reserve a portion of their privacy budget for future users to ask further queries of the data. Giving some of your budget to future users means less for depositors to spend now on releasing statistics that they know are important in their data but more flexibility for future users to ask the queries that are most relevant to their interests. 
</p>
<p>
In the bottom right corner of the interface there is a slider to control what percentage of the privacy budget is being reserved for future users. 
</p>
</div>


<div id="submit">
<h4>Submit and Show Buttons</h4>
<p>
In the footer of the tool are two buttons, the red <i>Submit Table</i> button, and the gray button titled <i>Show Underlying Table</i> (which may not appear in some versions).    
</p>
<p>
<b>Submit Table</b>
When you are finished selecting all of your statistics and are satisfied with the accuracies, click the Submit button. This will compute all of the differentially private statistics and generate a file for you to view. 
</p>
<p>
<b>Show Underlying Table</b>
As the tool cumulates information from the user, such as privacy loss parameters, requested statistics, metadata, desired accuracies, and released values, this information is built up into an underlying table describing the dataset.  This button allows this table to be inspected.  This table is rarely of direct interest, but is useful in presentations for demonstrating how some features of the prototype are working.
</p>
</div>

